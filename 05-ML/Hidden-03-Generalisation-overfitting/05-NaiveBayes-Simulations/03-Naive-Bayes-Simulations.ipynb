{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Naive-Bayes-Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will learn to simulate data to test hypotheses about algorithms. We will test two hypothesis:\n",
    "\n",
    "- (1) It is commonly accepted (check on the Internet) that performance of the algorithm Naive Bayes decreases when features are correlated. We will compare performances with correlated and not correlated feature. We will compare the performance to kNN as a baseline.\n",
    "- (2) We will test the time required to make prediction for kNN and Naive Bayes to see if kNN is slower.\n",
    "\n",
    "For this purpose, we will also see how to create data, which is a great way to experiment and see how algorithms behave. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Random Data with a Uniform Distribution\n",
    "\n",
    "Let's first import some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part, you will create a Numpy array filled with random values. These random values should be drawn from a *normal distribution*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can create the random array named `x1` with a distribution with a mean of 1 and a standard deviation of 0.7 and with 500 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try to plot the actual distribution of our array to be sure that it has the properties we asked for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "sns.distplot\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be a normal curve centered around 1.\n",
    "\n",
    "We want to create a fake *labeled* dataset. This means that we want to have one column containing continuous values (like our variable `x1` above) and the true labels: 0 or 1 since we want to do a binary classification. We need to have data with label 0 following a different distribution than the data with label 1. To do that, we will create another variable with a different distribution. It will correspond to the label 1.\n",
    "\n",
    "Your next task is to **create** another variable named `x2` drawn from a normal distribution with a **mean of 2** and a **standard deviation of 0.7**, with **500 values**.\n",
    "\n",
    "You will then **concatenate** `x1` and `x2` into a variable named `X1` (watch the uppercase) to end up with an array of shape `(1000, )`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "np.concatenate\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our first variable. We will now create the depend variable `y`: it is filled with 0 for the first distribution (the first 500 rows) and with 1 for the second distribution (the last 500 rows).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "np.repeat()\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have an array `y` of labels of 1000 rows: 500 `0` followed by 500 `1`.\n",
    "\n",
    "You will now create another feature that is *dependent* on the first feature `X1`.<br>\n",
    "To do that you will:\n",
    "- create an other normal distribution named `x3`, with a **mean of 0**, a **standard deviation of 0.2** and **1000 values**.\n",
    "- create a variable `X2` which is correlated to `X1`, `X2` will be the addition of `x3` and `X1` (not the concatenation).\n",
    "\n",
    "`x3` and `X2` must have the same shape than `X1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do a scatter plot with `X2` in function of `X1` to check that features are correlated. You can add colors for the label (the `y` value: 0 or 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also plot the distribution of `X1` or `X2` corresponding to a label of 0 along with the distribution of the samples corresponding to a label of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see two normal distributions: one centered in 1 and one centered in 2.\n",
    "\n",
    "The last thing to do to have our fake data is to stack `X1` and `X2` into a 2D Numpy array named `X`. This will be our input data. `X` should have 1000 rows and 2 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "np.vstack() and .T\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now shuffle the arrays `X` and `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "from sklearn.utils import shuffle\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling: Naive Bayes vs. kNN\n",
    "\n",
    "Now, you will try two algorithms on these data:\n",
    "\n",
    "- (1) kNN\n",
    "- (2) Naive Bayes\n",
    "\n",
    "For each model, use Sklearn and create the model, fit the data and calculate the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get an accuracy around 0.84 for the kNN and 0.75 for the Naive Bayes. You can see that Naive Bayes is not working as well as kNN.\n",
    "\n",
    "To see if this is better without correlations, we will create the variables `X1_ind` (for independent) and `X2_ind` like you created `X1` before. This time, `X2_ind` will not depend on `X1_ind`: it will be other random value drawn from a distribution with similar mean (1 for for label 0 and 2 for label 1). This time, the `X1_ind` and `X2_ind` variables will not be correlated. The variable `X_ind` will be the variables `X1_ind` and `X2_ind` stacked as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run again both classifier (kNN and Naive Bayes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, performance increased. This is because we added information with the second uncorrelated variable. This why it is important to compare these results with a baseline. We can see that the difference between kNN and Naive Bayes seems to be lower than with correlated feature. However, this difference is quite small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes vs. kNN for Computing Time\n",
    "\n",
    "There can be some advantages to use Naive Bayes instead of kNN. For instance, the prediction time should be lower with Naive Bayes. It can be game changing when you need to do prediction in real time for instance. Let's try to feed more data into our algorithms and calculate the time of prediction.\n",
    "\n",
    "You can create the variables `X1_big` and `X2_big` similar to `X1` and `X2` (independent) but with 100,000 values instead of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fit again a kNN and a Naive Bayes, and calculate the duration with the Python function `time`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "import time\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the functions `score` or `predict` take a lot more time with the kNN in comparison to the Naive Bayes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Optional\n",
    "\n",
    "To be able to plot the decision boundary of a model, you need to do a prediction on a grid and assign a color according to the prediction. It is a great tool to see how your model behaves.\n",
    "\n",
    "In this part, you will try to plot the data you generated along with the decision boundary of the models. You can plot the data and the decision boundary for:\n",
    "\n",
    "- kNN with correlated data\n",
    "- kNN with uncorrelated data\n",
    "- Naive Bayes with correlated data\n",
    "- Naive Bayes with uncorrelated data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "np.meshgrid()\n",
    "    \n",
    "ravel()\n",
    "\n",
    "np.arange()\n",
    "\n",
    "plt.pcolormesh()\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
