{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training Machine learning algorithms, preprocessed text needs to be transformed into numerical data. This process is called feature extraction, or vectorization. \n",
    "\n",
    "Run the code below to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>the happy bastard's quick movie review \\ndamn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            reviews\n",
       "0    neg  plot : two teen couples go to a church party ,...\n",
       "1    neg  the happy bastard's quick movie review \\ndamn ...\n",
       "2    neg  it is movies like these that make a jaded movi...\n",
       "3    neg   \" quest for camelot \" is warner bros . ' firs...\n",
       "4    neg  synopsis : a mentally unstable man undergoing ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "reviews = []\n",
    "\n",
    "for fileid in movie_reviews.fileids():\n",
    "    tag, filename = fileid.split('/')\n",
    "    reviews.append((tag, movie_reviews.raw(fileid)))\n",
    "\n",
    "df = pd.DataFrame(reviews)\n",
    "df.columns = ['target','reviews']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import your preprocessing work from the previous exercice and clean the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    plot two teen couple go church party drink dri...\n",
       "1    happy bastard quick movie review damn bug got ...\n",
       "2    movie like make jaded movie viewer thankful in...\n",
       "3    quest camelot warner bros first feature length...\n",
       "4    synopsis mentally unstable man undergoing psyc...\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "def clean (text):\n",
    "    \n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, ' ') # Remove Punctuation\n",
    "        \n",
    "    lowercased = text.lower() # Lower Case\n",
    "    \n",
    "    tokenized = word_tokenize(lowercased) # Tokenize\n",
    "    \n",
    "    words_only = [word for word in tokenized if word.isalpha()] # Remove numbers\n",
    "    \n",
    "    stop_words = set(stopwords.words('english')) # Make stopword list\n",
    "    \n",
    "    without_stopwords = [word for word in words_only if not word in stop_words] # Remove Stop Words\n",
    "    \n",
    "    lemma=WordNetLemmatizer() # Initiate Lemmatizer\n",
    "    \n",
    "    lemmatized = [lemma.lemmatize(word) for word in without_stopwords] # Lemmatize\n",
    "    \n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "\n",
    "df['cleaned'] = df['reviews'].apply(clean)\n",
    "\n",
    "df['cleaned'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the reviews in the dataset are too short to be considered for training. Others are too long. \n",
    "\n",
    "Keep only the reviews that are between 100 and 500 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(string):\n",
    "    tokens = string.split()\n",
    "    n_tokens = len(tokens)\n",
    "    return n_tokens\n",
    "\n",
    "df['count'] = df['cleaned'].apply(word_count)\n",
    "\n",
    "df = df[(df['count'] > 100) & (df['count'] < 500)]\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn's `CountVectorizer` has parameters that control the vectorizing transformations applied to the text prior to model training.  The different vectorizing transformations will themselves impact the result of the model. As such, it is important to fine tune the parameters of the vectorizer in relation to the model that follows.\n",
    "\n",
    "Read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to find out more about the following vectorizing parameters:\n",
    "- `ngram_range`\n",
    "- `max_df`\n",
    "- `min_df`\n",
    "- `max_features`\n",
    "\n",
    "Optimize those parameters with a Multinomial Naive Bayes model.\n",
    "\n",
    "You need to:\n",
    "\n",
    "- Initiate a pipeline made up of the `CountVectorizer` and Multinomial Naive Bayes model\n",
    "- Create a parameter dictionary for the `CountVectorizer`\n",
    "- Plug the pipeline and the parameters dictionary to a `GridSearch`\n",
    "\n",
    "[This](https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html) should help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.707\n",
      "Best parameters: {'nb__alpha': 0.5, 'vectorizer__max_df': 0.75, 'vectorizer__max_features': 50, 'vectorizer__min_df': 0.05, 'vectorizer__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())])\n",
    "\n",
    "parameters = {'vectorizer__ngram_range': [(1, 1),(2,2)],\n",
    "              'vectorizer__max_df':[0.33,0.5,0.75],\n",
    "              'vectorizer__min_df':[0.05,0.1],\n",
    "              'vectorizer__max_features' : [25,50,100],\n",
    "              \"nb__alpha\":[0.1,0.5,1]}\n",
    "\n",
    "gridsearch = GridSearchCV(pipeline, parameters, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "gridsearch.fit(df.cleaned, df.target)\n",
    "\n",
    "print( \"Best score:\", gridsearch.best_score_)    \n",
    "print( \"Best parameters:\", gridsearch.best_params_)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency (TfIdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than counting occurences as does the `CountVectorizer`, the `TfidfVectorizer` computes an importance value for each word in its text and according the entire corpus. That value is the product of the TF and the IDF.\n",
    "\n",
    "Following the same steps as with the `CountVectorizer`, tune your `TfidfVectorizer` [[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). \n",
    "\n",
    "This time, we also want you to fine tune the Multinomial Naive Bayes model's `alpha` parameter, which can be done within the same `GridSearch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.7195\n",
      "Best parameters: {'nb__alpha': 1, 'vectorizer__max_df': 0.75, 'vectorizer__max_features': 100, 'vectorizer__min_df': 0.05, 'vectorizer__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())])\n",
    "\n",
    "parameters = {'vectorizer__ngram_range': [(1, 1),(1,2),(2,2)],\n",
    "              'vectorizer__max_df':[0.33,0.5,0.75],\n",
    "              'vectorizer__min_df':[0.05,0.1],\n",
    "              'vectorizer__max_features' : [25,50,100],\n",
    "              \"nb__alpha\":[0.1,0.5,1]}\n",
    "\n",
    "gridsearch = GridSearchCV(pipeline, parameters, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "gridsearch.fit(df.cleaned, df.target)\n",
    "\n",
    "print( \"Best score:\", gridsearch.best_score_)    \n",
    "print( \"Best parameters:\", gridsearch.best_params_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
