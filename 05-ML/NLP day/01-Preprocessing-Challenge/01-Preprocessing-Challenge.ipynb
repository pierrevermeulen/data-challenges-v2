{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing consists of transforming raw text into a form that suits your task. The most basic transformations include removing punctuation, lower casing the text, removing numbers. More advanced are lemmatizing, remove stop words, Part of speech tagging. There are many more..\n",
    "\n",
    "It is important to chose the preprocessing steps that are suited to your task rather than systematically applying them. For example, if you are trying to predict the date of certain texts, numbers may be important.\n",
    "\n",
    "In this exercice you will learn some basic text preprocessing and apply it to the following sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = \"02/12/2018. The world loves Zlatan Ibrahimovic. He is such an amaZing Footballer!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function to remove punctuation and apply it to the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02 12 2018  The world loves Zlatan Ibrahimovic  He is such an amaZing Footballer  '"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string #This imports string.punctuation, a usable string made of all possible punctuation \n",
    "\n",
    "def remove_punctuation(text):\n",
    "    for punctuation in string.punctuation: \n",
    "        text = text.replace(punctuation, ' ') \n",
    "    return text\n",
    "\n",
    "stepbystep = remove_punctuation(example) \n",
    "\n",
    "stepbystep #We will apply the following functions to stepbystep to visualize changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function and apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02 12 2018  the world loves zlatan ibrahimovic  he is such an amazing footballer  '"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lowercase (text): \n",
    "    lowercased = text.lower() \n",
    "    return lowercased\n",
    "\n",
    "stepbystep = lowercase(stepbystep) #Remember, apply function to stepbystep\n",
    "\n",
    "stepbystep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing splits up a text into a list of individual words, also called tokens. Using word_tokenize, complete function. Apply to stepbystep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02',\n",
       " '12',\n",
       " '2018',\n",
       " 'the',\n",
       " 'world',\n",
       " 'loves',\n",
       " 'zlatan',\n",
       " 'ibrahimovic',\n",
       " 'he',\n",
       " 'is',\n",
       " 'such',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'footballer']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize \n",
    "\n",
    "def tokenize (text):\n",
    "    tokenized = word_tokenize(text)\n",
    "    return tokenized\n",
    "\n",
    "stepbystep = tokenize(stepbystep)\n",
    "\n",
    "stepbystep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function to remove numbers using .isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'world',\n",
       " 'loves',\n",
       " 'zlatan',\n",
       " 'ibrahimovic',\n",
       " 'he',\n",
       " 'is',\n",
       " 'such',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'footballer']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_numbers (text):\n",
    "    words_only = [word for word in text if word.isalpha()]\n",
    "    return words_only\n",
    "\n",
    "stepbystep = remove_numbers(stepbystep)\n",
    "\n",
    "stepbystep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Stopwords\" are words are so frequently used that for many tasks (but not all), they don't carry much information. Examples are \"any\", \"all\", \"what\"... NLTK has an inbuilt corpus of english stopwords that can be loaded and used. \n",
    "\n",
    "Using that corpus, create a list of english stopwords. Then, complete function to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['world', 'loves', 'zlatan', 'ibrahimovic', 'amazing', 'footballer']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Create a list of english stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# Create function\n",
    "def remove_stopwords (text):\n",
    "    without_stopwords = [word for word in text if not word in stop_words]\n",
    "    return without_stopwords\n",
    "\n",
    "stepbystep = remove_stopwords(stepbystep)\n",
    "\n",
    "stepbystep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing consists of reducing word derivatives down to their ethymological roots. For example: studies & studying --> study.\n",
    "\n",
    "Complete the two parts in the following function. First, initiate a WordNetLemmatizer using the NLTK package. Then use it to lemmatize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['world', 'love', 'zlatan', 'ibrahimovic', 'amazing', 'footballer']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer() # Initiate lemmatizer\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in text] # Lemmatize\n",
    "    return lemmatized\n",
    "\n",
    "stepbystep = lemmatize(stepbystep)\n",
    "\n",
    "stepbystep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all of the above steps into one single function, then apply it to the original sentence \"example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean (text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, ' ') # Remove Punctuation\n",
    "    lowercased = text.lower() # Lower Case\n",
    "    tokenized = word_tokenize(lowercased) # Tokenize\n",
    "    words_only = [word for word in tokenized if word.isalpha()] # Remove numbers\n",
    "    stop_words = set(stopwords.words('english')) # Make stopword list\n",
    "    without_stopwords = [word for word in words_only if not word in stop_words] # Remove Stop Words\n",
    "    lemma=WordNetLemmatizer() # Initiate Lemmatizer\n",
    "    lemmatized = [lemma.lemmatize(word) for word in without_stopwords] # Lemmatize\n",
    "    return lemmatized\n",
    "\n",
    "cleaned_in_one_go = clean(example) # Apply the \"clean\" function to our original sentence \"example\"\n",
    "\n",
    "cleaned_in_one_go"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
