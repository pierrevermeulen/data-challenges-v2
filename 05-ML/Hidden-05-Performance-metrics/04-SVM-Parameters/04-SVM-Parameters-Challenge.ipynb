{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Machine Learning algorithm has specific parameters that affect its performance. For Support Vector Machines, the main parameters are:\n",
    "- 'C'\n",
    "- 'Kernel'\n",
    "- 'Gamma'\n",
    "\n",
    "You already know that 'C' manages the error function according to the Support Vectors.\n",
    "\n",
    "In this exercice, you will explore the other two parameters.\n",
    "\n",
    "Run the following code. It imports the packages, data, and creates plotSVM(), a function you will use to visualize parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "iris = datasets.load_iris() # Load dataset as \"iris\"\n",
    "\n",
    "X = iris.data[:, :2] # Keep only 2 first features, Sepal Lenght and Sepal Width\n",
    "\n",
    "y = iris.target # Load targets\n",
    "\n",
    "def plotSVM(title): \n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 #From Sepal lenght (X[0])Set X axis min to lowest data point -1, X axis max to highest data point +1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 #From Sepal Width (X[1])Set Y axis min to lowest data point -1, Y axis max to highest data point +1\n",
    "    h = 0.01 #Set delimiting line width \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) #Create rectangular grid out of two one dimensional arrays\n",
    "    \n",
    "    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()]) #Fit data to SVM\n",
    "    Z = Z.reshape(xx.shape) #Convert to compatible 2D array shape\n",
    "    \n",
    "    plt.subplot(1, 1, 1) #Set dimensions of plot\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.tab20) #Show delimiter and area and pick color\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.tab20) #Show data points and pick color\n",
    "    plt.xlabel('Sepal length') #Label X axis\n",
    "    plt.ylabel('Sepal width') # Label y axis\n",
    "    plt.title(title) #Print Title\n",
    "    \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kernel parameter defines which type of hyperplane separates the data.\n",
    "\n",
    "In the case of 2D data, a linear hyperplane is used. Remember when you plotted the Support Vectors in exercice 02? The data was separable by a single line.\n",
    "\n",
    "In the case of more complex data, Radial Basis Function (rbf) or polynomial hyperplanes will typically do a better job, as they can take more complex shapes.\n",
    "\n",
    "Make a list of the 3 kernel names as they are referred to in the [[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the different kernels to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following for-loop to train and plot a model for each kernel. The `plotSVM()` function takes the kernel name as argument. Your data is the X and y defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kernel in kernels: \n",
    "    \n",
    "    # Initiate SVC and fit data\n",
    "    \n",
    "    # Plot using plotSVM function, and use corresponding kernel as plot title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code from exercice 03, grid search the best Kernel for your data. Follow these steps:\n",
    "- Create kernel parameter dictionary\n",
    "- Initiate gridsearch\n",
    "- Fit gridsearch to data\n",
    "- Unpack the parameter that produces the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gamma parameter is used for non linear hyperplanes. The higher its value, the more complex its shape to try fit the training data. Be careful, this can lead to overfitting. The best tradeoff can be found via grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn to plot! Using the same steps as for the Kernel parameter, plot for the following Gamma values:\n",
    "- [10, 1, 1e-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, which value of Gamma appears to be overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search the Gamma values and print out best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
