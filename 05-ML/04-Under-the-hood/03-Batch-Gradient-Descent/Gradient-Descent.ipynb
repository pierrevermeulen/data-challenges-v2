{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercice, you will create the necessary functions to go through the steps of a single Gradient Descent Epoch. You will then combine the necessary functions and create a loop through the entire Gradient Descent procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Import the dataset located in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Visualize the relation between the variables `Phosphorus (mg/100g)` and `Zinc (mg/100g)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization should hint at a somewhat Linear relationship between the variables. Let's use Gradient Descent to find the line of best fit between them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Before you start, scale the two features with a `MinMaxScaler`. This will allow the Gradient Descent to be more efficient and converge faster. Add the scaled features as new columns in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Create the two `pd.Series` `x` and `y` as `zinc` and `phosphorus` respectively (their scaled version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code one Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the exercice, you will define the key functions used to update the parameters during one epoch (k) of gradient descent. Recall the formula below\n",
    "\n",
    "$$\n",
    "\\beta_0^{\\color {red}{(k+1)}} = \\beta_0^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\beta_0}(\\beta^{\\color{red}{(k)}})\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\beta_1^{\\color {red}{(k+1)}} = \\beta_1^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\beta_1}(\\beta^{\\color {red}{(k)}})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} =  a x + b\n",
    "$$\n",
    "\n",
    "üëá Define the hypothesis function of a Linear Regression. Let `a` be the slope and `b` the intercept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x,a,b):\n",
    "    # YOUR CODE HERE\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What would be your predicted amount of phosphorus if:\n",
    "- zinc = 0.1\n",
    "- a = 1\n",
    "- b = 1\n",
    "\n",
    "Use your hypothesis function to compute the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è If the answer is not `1.1`, something is wrong with your function. Fix it before moving on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Least\\ Squares\\ Loss = \\sum_{i=0}^n (y^{(i)} - \\hat{y}^{(i)} )^2\n",
    "$$\n",
    "\n",
    "üëá Define the Least Squares Loss Function for the above created Hypothesis Function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "You must use the Hypothesis Function within the Loss function to compute the predictions at given parameter values.\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x,y,a,b):\n",
    "    # YOUR CODE HERE\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What would be the total Loss if:\n",
    "- a = 1 \n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è You should be getting 63.86. If not, something is wrong with your function. Fix it before moving on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial SSR}{\\partial slope}= \\sum_{i=0}^n -2(y^{(i)} - \\hat{y}^{(i)} )\\times x\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial SSR}{\\partial intercept}= \\sum_{i=0}^n -2(y^{(i)} - \\hat{y}^{(i)} ) \n",
    "$$\n",
    "\n",
    "üëá Define a function to compute the partial derivatives of the Loss Function for each parameter `a` and `b` at given points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Again, you must use the Hypothesis Function within to compute the predictions at given points.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x,y,a,b):\n",
    "    # YOUR CODE HERE\n",
    "    return (derivative_a, derivative_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Using your function, what would be the partial derivatives of each parameter if:\n",
    "- a = 1\n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è You should be getting 48.45 and  115.17. If not, fix your function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "step\\ size = gradient \\cdot learning\\ rate\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Define a function that calculates the step sizes alongside each parameter (a,b), according to their derivatives (derivative_a, derivative_b) and a learning_rate equals to 0.01 by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steps(derivative_a, derivative_b, learning_rate = 0.01):\n",
    "    # YOUR CODE HERE\n",
    "    return (step_a, step_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What would be the steps to take for the derivatives computed above for (a,b) = (1,1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è The steps should be 0.48 for a and 1.15 for b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "updated\\ parameter = old\\ parameter\\ value - step\\ size\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Define a function that computes the updated parameter values from the old parameter values and the step sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_params(a, b, step_a, step_b):\n",
    "    # YOUR CODE HERE\n",
    "    return updated_a , updated_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Using the functions you just created, compute the updated parameters at the end of the first Epoch, had you started with parameters:\n",
    "- a = 1\n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è You should be getting the following values:\n",
    "   - updated_a = 0.51\n",
    "   - updated_b = -0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Now that you have the necessary functions for a Gradient Descent, loop through epochs until convergence.\n",
    "\n",
    "- Initialize parameters `a = 1` and  `b = 1`\n",
    "- Consider convergence to be **100 epochs**\n",
    "- Don't forget to start each new epoch with the updated parameters\n",
    "- Append the value of the loss, a, and b at each epoch to a list called `loss_history`, `a_history` and `b_history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a and b\n",
    "\n",
    "\n",
    "# Loop through steps to perform Gradient Descent\n",
    "    \n",
    "    # Compute Loss at each Epoch and append to loss_history\n",
    "\n",
    "    # Compute the gradient (the two partial derivatives) \n",
    "    \n",
    "    # Compute steps\n",
    "    \n",
    "    # Compute updated parameters\n",
    "    \n",
    "    # Set updated parameters for new epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What are the parameter values for `a` and `b` at the end of the 100 epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be getting 0.76 and 0.007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Wrap this iterative approach into a method `gradient_descent()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, a_init=1, b_init=1, learning_rate=0.001, n_epochs=100):\n",
    "    # YOUR CODE HERE\n",
    "    return a_new, b_new, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Plot the line of best fit through Zinc and Phosphorus using the parameters of your Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Plot your gradient as a 2D-surface using matplotlib [contourf](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.contourf.html).  \n",
    "Plot on the same figure all historical (a,b) points as red dots to visualize your gradient descent!\n",
    "Change your learning rate and observe it's impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "\n",
    "You'll have to create \n",
    "- a range of values for `a`\n",
    "- a range of values for `b`    \n",
    "- a 2D-array `Z` where each elements `Z[i,j] = loss(x,y,a[i],b[j])`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá (optional) try also it to visualize your gradient with the interactive plot.ly [3D contour plot](https://plotly.com/python/3d-surface-plots/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Plot the Loss history as a function of epochs. Vary the learning_rate from 0.001 to 0.01. Notice the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. With Sklearn..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Using Sklearn, train a Linear Regression model on the same data. Compare its parameters to the ones computed by your Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They should be almost identical!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†‚ö†Ô∏è Please, push your exercice when you are done üôÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÅ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
