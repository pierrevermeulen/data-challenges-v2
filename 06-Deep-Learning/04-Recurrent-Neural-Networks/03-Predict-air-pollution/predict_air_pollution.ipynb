{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning week - Day 4 - Predict Air Pollution\n",
    "\n",
    "### Exercise objectives\n",
    "- Prepare the data\n",
    "- Further dig into Recurrent Neural Networks\n",
    "- Stack multiple layers of RNNs\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "In this exercise, you will predict the pollution (measured as a number of particles) on the next day given a sequence of weather features, such as the temperature, the pression, etc.\n",
    "\n",
    "In real-life applications, the data are not as well-prepared as in the previous exercises. For this reason, the first steps of the notebook correspond to the data preparation.\n",
    "\n",
    "Then, given your new RNN ninja skills and the fact that the exercise is similar to previous challenges, less help is given as how to write a RNN. This can happen in real-life problems where you will always be able to get back to Le Wagon exercise to copy-paste what you have done to start working.\n",
    "\n",
    "\n",
    "<hr><hr>\n",
    "\n",
    "# Data\n",
    "\n",
    "The data here corresponds to hourly measurements of the air pollution (feature: `pm2.5`, which is the concentration of 2.5 millimeter particles) that you will try to predict. Among the other related features, you have:\n",
    "- TEMP: Temperature\n",
    "- DEWP: Dew Point\n",
    "- PRES: Pressure\n",
    "- Ir: Cumulated hours of rain\n",
    "- Iws: Cumulated wind speed\n",
    "- Is: Cumulated hours of snow\n",
    "\n",
    "❓ **Question** ❓ Load the data `data.txt` - use the first column as the index of a panda Dataframe.\n",
    "Let's consider only the features presented above (pm2.5, TEMP, DEWP, PRES, Ir, Iws and Is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Plot the temporal progression of the different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Let's normalize the variables `pm2.5` and `PRES` as their value can get very high. Just divide their values by a factor 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercises, we had multiple independent data sequences. Here, you notice that there is only one. And this is quite often the case. So how to deal with such data? In fact, this long sequence can be separated in a lot of short sequences that we will consider as independant. \n",
    "\n",
    "❓ **Question** ❓ Write a function that, given the initial dataframe, return a shorter dataframe sequence of length `length`. This shorter sequence should be selected at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_sequence(df, length):\n",
    "    ### YOUR CODE HERE\n",
    "    return df_sample\n",
    "\n",
    "df_subsample = subsample_sequence(df, length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Write a function that given a full dataframe, first subsample this dataset into a shorter sequence, and then splits the shorter dataframe into a training sequence and a value to predict.\n",
    "\n",
    "Basically, if your sub-sampled dataframe is of size N, you will take the features during the N-1 first days as your variables `X`, and the value of the pollution at day N as your variable `y`.\n",
    "\n",
    "❗ **Remark**❗ There are missing values in the dataframe. If the value to predict `y` is missing, the function should rerun. If there are missing values in the variables `X`, then it should be replaced by the mean values over the other selected hours. If all the other hours are missing, then they should all be replaced by the mean value of the dataframe.\n",
    "\n",
    "❗ **Remark**❗ The outputs should be arrays or list, not a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_subsample_sequence(df, length):\n",
    "    df_subsample = subsample_sequence(df, length)\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    return X_subsample, y_subsample\n",
    "\n",
    "df_subsample = split_subsample_sequence(df, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Thanks to the previous function, write another function that generates an entire dataset $(X, y)$ of multiple subsamples, given an initial dataframe `df`, a number of desired sequences, and a `length` for each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(df, number_of_sequences, length):\n",
    "    ### YOUR CODE HERE\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Generate a dataset $(X, y)$ with consists of 100 sequences, each of 20 observations - the value of the pollution at the 21-st day being the value to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Check the shape of your inputs. `X` should be of size (100, 20, 7) ( => (sequences, length, number of features) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗ **IMPORTANT REMARK: POTENTIAL DATA LEAKAGE**❗ If you split this dataset (X, y) into a training and a test set, it is much likely that some data in the train set are in the test set. Especially, you will predict data in test that are input data in train.\n",
    "\n",
    "To avoid this situation, you should _first_ separate your initial dataframe `df` into a training dataframe  and test dataframe.\n",
    "\n",
    "❓ **Question** ❓ Separate `df` into `df_train` and `df_test` such that the first 80% of the dataframe are in the training. And the last 20% in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Now, you can generate (X_train, y_train) from df_train and (X_test, y_test) from df_test.\n",
    "The training test should correspond to 1000 sequences, each of size 50 (+ the time to predict). The test set should correspond to 200 sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Initialize a model the way you want and compile it within the `init_model` function. _TRY_ to do it, before looking at the previous exercise.\n",
    "Start here with a simple `LSTM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    ### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Fit your model and evaluate it on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Compare your prediction to a benchmark prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack RNN layers\n",
    "\n",
    "❓ **Question** ❓ Now that you know how to write a recurrent architecture, let's see how to stack one.\n",
    "If you want to stack multiple RNN, LSTM, GRU layers, it is very easy. Do it as if they were Dense (or any other) layers.\n",
    "\n",
    "But don't forget: All RNN (**EXCEPT** the last one) should have the `return_sequences` set to True so that the entire sequence of predictions of a given layer is given to the next layer. Otherwise, you will only give the last prediction to the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_2():\n",
    "    ### YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Evaluate your new model to the previous prediction and to the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Now, let's see how the performance changes depending on the number of seen days we used to sample our initial splits (50 days in the previous example).\n",
    "\n",
    "For different values of temporal sequence lengths, resplit your data, run your model and evaluate its performance (do not forget to reinitialize your model between each run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
