{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Day 2 - Playground Part II\n",
    "\n",
    "### Exercise objectives:\n",
    "- Write a full Neural Network on your own\n",
    "- Better understand Neural Network hyperparameters\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "Yesterday, we explore part of the [Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.03295&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&regularizationRate_hide=true) to learn about Neural Network. \n",
    "\n",
    "First, as the algorithm is stochastic, the results might differ from one run to the other. For that reason, do not hesitate to rerun the algorithms multiple times to be sure of your deduction and reasonings.\n",
    "\n",
    "### Let's explore the different things we have seen  during the lectures.\n",
    "\n",
    "\n",
    "## 1. The batch size\n",
    "\n",
    "❓ **Question** ❓ Select the circle dataset (classification). Build a model with one hidden layer with 3 neurons, a learning rate of 0.03 and the tanh activation function. Do not put any noise (=0).\n",
    "\n",
    "❓ **Question** ❓ Select a batch size of 30 and look at the convergence of the algorithm. Does it seem slow or fast?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Now, run the same neural network on the same data but with a batch-size of 1? Be sure to run at least 150 epochs. What do you notice on the train and test loss? What is the reason of this instability? \n",
    "\n",
    "❓ **Question** ❓ Now, you can see the effect by reading the values of the train and test loss: pause the iterations and run it step by step (iteration per iteration) thanks to the \"Step\" button (at the right side of the play/stop button)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ To observe once again the lack of generalization for small batch sizes, select the \"Exclusive or\" dataset. Use a noise of 50. Keep the same neural network as before, but with 8 neurons on the hidden layer. Start with a batch size of 30, and then, compare it to a run with a batch size of 1. Look especially at the test loss. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Change now to the spiral dataset, with a ratio of training to test data of 80%. For the neural network, use 3 hidden layers with 8 neurons on the first layer, 7 on the second and 6 on the third. Run the algorithm with a batch size of 30. Be sure to run it for at least 1500 epochs. Then, compare it to the same run but with a batch size of 1. You can check what happens on the train and test loss step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The learning rate\n",
    "\n",
    "Go back to the circle dataset with no noise and a ratio of training to test data of 50%. Use a batch size of 20. \n",
    "\n",
    "Use a neural netwok with one layer of 5 neurons (no regularization, and the tanh activation function)\n",
    "\n",
    "❓ **Question** ❓ For each learning rate (from 0.00001 to 10), run the algorithm during 1000 epochs and report the values of the test loss in the list below. Then, plot the test loss with respect to the learning rates. \n",
    "\n",
    "⚠️ Warning ⚠️ When you change the learning rate, be sure to reinitialize the neural network (circular arrow, left to the play/pause button)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "learning_rates = [0.00001, 0.0001, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "test_loss = [### YOUR LOSS VALUES]\n",
    "\n",
    "\n",
    "plt.plot(np.log(learning_rates), test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Warning ⚠️ It is important to understand that, even though small and high learning rates have high test loss, this is absolutely not for the same reason. On the one hand, small learning rates do converge as middle learning rate, but way slower. If you would like, you can try to run it all night long to see that it finally converge to the same test loss but with much more epochs. On the other hand, large learning rates make the algorithm diverge.\n",
    "\n",
    "\n",
    "❓ **Question** ❓ To convince yourself of the lack of convergence of high learning rates, select a learning equal to 10 and run it 10 times, each time with 400 epochs. Report the values in the following list to see the variability of values, which corresponds to the fact that the algorithms converge to different local minima (which is also to be seen on the final prediction which is each time different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = [###YOUR LOSS VALUES]\n",
    "\n",
    "plt.boxplot(test_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ You can play around with different dataset and make the noise vary. Look at the effect of different learning rate. You can especially change the learning rate _during_ the learning to see how it affects the train and test losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gaussian data\n",
    "\n",
    "❓ **Question** ❓ Use the gaussian dataset with a noise of 15, and a ratio of training to test data of 50%. Build any model you want that separates the data.\n",
    "\n",
    "❓ **Question** ❓ Now, write this model in Keras - do not forget to cimpile it with an optimizer and the appropriate loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Let's reproduce this data. Can you plot them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_1 = np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], 300)\n",
    "X_2 = np.random.multivariate_normal([3, 3], [[1, 0], [0, 1]], 300)\n",
    "X = np.concatenate((X_1, X_2))\n",
    "\n",
    "y_1 = [0]*len(X_1)\n",
    "y_2 = [1]*len(X_2)\n",
    "y = np.array(y_1 + y_2)\n",
    "\n",
    "X, y = shuffle(X, y)\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Separate the data between test and train set (50/50 ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Train the model on the train data. Do not forget to choose an appropriate batch_size, which can be the same you use for your Tensorflow Playground Neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Plot the model loss with respect to the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Evaluate your score on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulation! You are now able to fully write a Neural Network of your own. Easy, no?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
