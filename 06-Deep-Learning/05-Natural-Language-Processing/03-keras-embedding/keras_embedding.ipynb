{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Day 5 - Keras embedding\n",
    "\n",
    "### Exercise objectives:\n",
    "- Learn embeddings with Keras\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "In the previous exercise, we used the word2vec algorithm to embed the input words, meaning that for each word, we find a vectorial representation that was used as input of a RNN.\n",
    "\n",
    "However, this vectorial representation was not designed to specifically handled a specific task. For that reason, we will here use an embedding layer in our RNN so that each word is represented in an embedding space whose values are specifically  learnt for our task. For instance, for the word `dog` which is represented by the vector $(w_1, w_2, ..., w_n)$ in the embedding space, we will learn the weights $(w_k)_k$.\n",
    "\n",
    "# The data\n",
    "\n",
    "\n",
    "❓ **Question** ❓ Let's first load the data. You don't have to understand what is going on in the function, it does not matter here.\n",
    "\n",
    "\n",
    "⚠️ **Warning** ⚠️ The `load_data` function has a `percentage_of_sentences` argument. Depending on your computer, there are chances that a too large number of sentences will make your compute slow down, or even freeze - your RAM can even overflow. For that reason, you can start with 20% of the sentences and see if your computer handles it. Otherwise, rerun with a lower number. On the other hand, you can increase the number if you feel like it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    # Load the data\n",
    "    (sentences_train, y_train), (sentences_test, y_test) = imdb.load_data()\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "        len_train = int(percentage_of_sentences/100*len(sentences_train))\n",
    "        sentences_train = sentences_train[:len_train]\n",
    "        y_train = y_train[:len_train]\n",
    "        \n",
    "        len_test = int(percentage_of_sentences/100*len(sentences_test))\n",
    "        sentences_test = sentences_test[:len_test]\n",
    "        y_test = y_test[:len_test]\n",
    "            \n",
    "    # Load the {interger: word} representation\n",
    "    word_to_id = imdb.get_word_index()\n",
    "    word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "    for i, w in enumerate(['<PAD>', '<START>', '<UNK>', '<UNUSED>']):\n",
    "        word_to_id[w] = i\n",
    "\n",
    "    id_to_word = {v:k for k, v in word_to_id.items()}\n",
    "\n",
    "    # Convert the list of integers to list of words (str)\n",
    "    X_train = [' '.join([id_to_word[_] for _ in sentence[1:]]) for sentence in sentences_train]\n",
    "    X_test = [' '.join([id_to_word[_] for _ in sentence[1:]]) for sentence in sentences_test]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "### Just run this cell to load the data\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "### Conversion to list of list of words\n",
    "\n",
    "❓ **Question** ❓ The first step of the data cleaning is the same as previously. Convert your list of sentences to list of list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "You probably haven't notice but Word2Vec handles word directly. In fact, it tokenizes the words internally, meaning that it creates a one-to-one relation between each word in the train set and a integers, thanks to a dictionarly. \n",
    "\n",
    "You will need to do the tokenization by yourself here.\n",
    "\n",
    "❓ **Question** ❓ Create a dictionary `word_to_id` such that each word ot `X_train` is in this dictionnary. The value of each key (=word) should be a unique integer. At the and, each word is represented by a unique integer - and vice-versa! \n",
    "\n",
    "⚠️ **Warning** ⚠️ **DO NOT USE 0 as a key in your dictionary. Start at 1!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ The number of keys should be equal to the number of different words in the train sentences. Print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Currently, your sentences are like ['this', 'is', 'a', 'very, 'very', 'good', 'movie\"]. Thanks to the `word_to_id` dictonary, replace them their tokenized representation : [3, 89, 2, 13, 13, 56, 32] (example, not true). Store them in `X_token_train` and `X_token_test`\n",
    "\n",
    "❗ **Remark** ❗ It is better writting a function to do that. And don't forget that there is no reason all the words in the test sentences to be in this dictionary - in that case, you can skip the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "❓ **Question** ❓ Pad your sentences to be RNN-ready! Here, pad your values with 0 (which is the default value)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model\n",
    "\n",
    "❓ **Question** ❓ Write a model that has:\n",
    "- an embedding layer whose `input_dim` is the size of your vocabulary (should be an argument of the function), and whose `output_dim` is the size of you embedding space - you can choose 50 as in the word2vec, or anything you want\n",
    "- a RNN (SimpleRNN, LSTM, GRU) layer\n",
    "- a Dense layer\n",
    "- an output layer\n",
    "\n",
    "⚠️ **Warning** ⚠️ Here, you don't need a masking layer. Why? Because `layers.Embedding` has a argument to do that directly `mask_zero=True`. This is the reason why `id_to_word` should not have a word that is represented by 0. Otherwise, the Embedding layer would remove it\n",
    "\n",
    "Compile it with the appropriate arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(vocab_size):\n",
    "    ### YOUR CODE HERE\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Fit your model on training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Warning** ⚠️  You probably don't want to spend the rest of your week-end running this model. One of the reason is that there are too many parameters, due to the fact that you embeded a lot of different words.\n",
    "\n",
    "**JUST STOP THE MODEL FIT AS FOR NOW!**.\n",
    "\n",
    "❓ **Question** ❓ Look at the number of parameters in your model. What do you think about it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer learns an embedding per word. Thus, if you set the embedding space size to be 50 in the RNN, it will learn 50 weights per word. If you have 10.000 differents words, the embedding has 500.000 parameters... Which is wayyy to long to optimize.\n",
    "\n",
    "❓ **Question** ❓ To reduce the number of words, rerun function on `X_train` and `X_test`. But keep only words that occurs more than 30 times (30 is an example, you can take more, but it is a good starting point to keep your training quick). Then, tokenize once again your sentences and display the number of words in your vocabulary.\n",
    "\n",
    "\n",
    "❗ **Remark** ❗ Be careful, some reviews have all their words below some given number of occurences. When you skip the given words, you may end up with sentences without any word in it. You have to handle that, as you cannot have an empty $X$. In the case it happens, advice is given to replace by a vector of zeros for instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Let's accelerate here the convergence of the model. The computational time depends on\n",
    "- the batch size\n",
    "- the number of training observation\n",
    "- the padded length of the sequences\n",
    "- the size of each observation, which here, is 1 because each word is encoded within one token\n",
    "- the number of parameters of the Neural network\n",
    "\n",
    "Let's here work on the padded length of your sequences! Instead of padding up to the maximal length within your training set, use the `maxlen` argument (i.e. maximal length) in `pad_sequences`. The idea here is to set `maxlen` to a number between 200 and 300 so that shorter sentences are padded up to this `maxlen` number of words. On the other hand, longer sentences are shorten. It fasten drastically the computational time. The rational is that, even with 200 to 300 words, you can classify whether the sentence is positive or negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ You can now rerun your model (and check the number of parameters). Then evaluate it your test set. You can go to the next questions while waiting for it to converge - do not make it more than 10 epochs.\n",
    "\n",
    "To make it run faster, you can take a larger batch_size if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model - more advanced.\n",
    "\n",
    "Let's train some fancy network here. \n",
    "Each of your word is represented by a vector of size N (the size of your embedding). Therefore, as a sentence is a sequence of words, it is represented by a matrix (number of words, N). So, all your sentences are actually represented as matrices once embeded.\n",
    "\n",
    "If you think about it, an image is also a matrix. Said differently, you may represent your sentence of word as a matrix, where each column (or row, depending on how you want to look at it) is a word, and each row (or each column) corresponds to a coordinate in the embedding space.\n",
    "\n",
    "Well, in that case, as these are close to images, why not using convolution on them? Yes, convolutions!\n",
    "But, be careful. In the case of images, convolutions are 2 dimensional as the filters can move up and down, and left and right. In the case of our sentences, we want the corresponding kernel to move _only_ in the word by word direction (otherwise, moving coordinate of the embedding space by coordinate doesn't make much sense).\n",
    "\n",
    "So let's create a model that use convolution\n",
    "\n",
    "### First, the data\n",
    "\n",
    "❓ **Question** ❓ In the case of convolutions, the input images must be of the same size. Pad them with a value `maxlen` equal to 150 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using 1D Convolution.\n",
    "\n",
    "❓ **Question** ❓ Define a model that has :\n",
    "- an Embedding layer: `input_dim` is the vocab_size, `output_dim` is the embedding space dimension, and `mask_zero` has to be set to true. Here, for computational reasons, set `input_length` to the maximum length of your observations (that you just defined in the previous question).\n",
    "- a conv1D layer \n",
    "- a Flatten layer\n",
    "- a dense layer\n",
    "- an output layer\n",
    "\n",
    "Compile the model accordingly\n",
    "\n",
    "❗ **Remark** ❗ The size of the Conv1D kernel corresponds exactly to the number of side-by-side words each kernel is taking into account ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cnn_model(vocab_size):\n",
    "    ### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Look at the number of parameters and compare it to the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Fit your model with a stopping criterion, and evaluate it on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
