{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Day 5 - Understand embeddings with Word2Vec\n",
    "\n",
    "### Exercise objectives:\n",
    "- Convert words to vector representations thanks to embeddings\n",
    "- Discover the powerful Word2Vec algorithm\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "Natural Language Processing corresponds to the automated study of language. To deal with texts, we have to convert them into meaningful representations that can be handled by computers. \n",
    "\n",
    "Usually, the first step consists of cleaning the data. As you have already done it during the past weeks, we won't emphasize this part today. However, if you have finished an exercise and you feel confortable with what you have, you are welcome to improve the data cleaning to achieve better performances.\n",
    "\n",
    "✅ **Good Practice** ✅ Never spend too much time on data cleaning! First, build the entire pipeline to have a baseline evaluation of your task. **Then, and only then, improve your data cleaning and _watch_ if it is really improving your score**. Otherwise, if you work on your data cleaning without baseline evaluation of your performances, you don't know whether your fancy data cleaning is improving the entire pipeline or not. And sometimes, it can get super counterintuitive ;)\n",
    "\n",
    "In this exercise, a large part of the data cleaning is already done. The next step is to convert each word of our vocabulary to a vector that can be fed to a Recurrent Neural Network. To do that, you will use the powerful and well-known Word2Vec algorithm.\n",
    "\n",
    "\n",
    "# The data\n",
    "\n",
    "Keras provides many datasets, among which the ÌMDB dataset: it corresponds to sentences that are movie reviews. Each of them is related to a score given by the review writer.\n",
    "\n",
    "❓ **Question** ❓ Let's first load the data. You don't have to understand what is going on in the function, it does not matter here.\n",
    "\n",
    "\n",
    "⚠️ **Warning** ⚠️ The `load_data` function has a `percentage_of_sentences` argument. Depending on your computer, there are chances that a too large number of sentences will make your compute slow down, or even freeze - your RAM can even overflow. For that reason, you can start with 20% of the sentences and see if your computer handles it. Otherwise, rerun with a lower number. On the other hand, you can increase the number if you feel like it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    # Load the data\n",
    "    (sentences_train, y_train), (sentences_test, y_test) = imdb.load_data()\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "        len_train = int(percentage_of_sentences/100*len(sentences_train))\n",
    "        sentences_train = sentences_train[:len_train]\n",
    "        y_train = y_train[:len_train]\n",
    "        \n",
    "        len_test = int(percentage_of_sentences/100*len(sentences_test))\n",
    "        sentences_test = sentences_test[:len_test]\n",
    "        y_test = y_test[:len_test]\n",
    "            \n",
    "    # Load the {interger: word} representation\n",
    "    word_to_id = imdb.get_word_index()\n",
    "    word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "    for i, w in enumerate(['<PAD>', '<START>', '<UNK>', '<UNUSED>']):\n",
    "        word_to_id[w] = i\n",
    "\n",
    "    id_to_word = {v:k for k, v in word_to_id.items()}\n",
    "\n",
    "    # Convert the list of integers to list of words (str)\n",
    "    X_train = [' '.join([id_to_word[_] for _ in sentence[1:]]) for sentence in sentences_train]\n",
    "    X_test = [' '.join([id_to_word[_] for _ in sentence[1:]]) for sentence in sentences_test]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "### Just run this cell to load the data\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have loaded the data, let's look at it!\n",
    "\n",
    "❓ **Question** ❓ `X_train` and `X_test` are lists of sentences. For each of them, let's plot some of the sentences with their respective scores stored in `y_train` and `y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probaly see that the data have already been cleaned a little. \n",
    "\n",
    "Moreover, the task corresponds to a binary classification problem:\n",
    "- label 0 corresponds to a negative review\n",
    "- label 1 corresponds to a positive review\n",
    "\n",
    "As for now, your sentences correspond to only a long string, but your neural network does not know that this string is composed of different words. For that reason, you must convert each sentence (the full string) into a list words, i.e. a list of separated strings.\n",
    "\n",
    "For instance, 'this is a good movie' should become ['this', 'is', 'a', 'good', 'movie'].\n",
    "\n",
    "❓ **Question** ❓ Write a function to convert `X_train` and `X_test` from list of strings (sentences) to list of list of strings (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is impossible to use strings into Neural Networks. Therefore, we have to convert each word into a mathematical representation, namely a vector. By the way, if a word is represented by a vector of size N, then a sentence of M words is represented by a vector of size M x N.\n",
    "\n",
    "All your words will then be vector (or points) in an N-dimensional space. This space is called the embedding space. And there are many ways to create this embedding space. Let's discover it.\n",
    "\n",
    "\n",
    "# Embedding with Word2Vec\n",
    "\n",
    "Now, let's use Word2Vec to embed the words of our sentences. Word2Vec will be able to convert each word to a fixed-size vectorial representation.\n",
    "\n",
    "For instance, we will have:\n",
    "- 'dog' --> [0.1, -0.3, 0.8]\n",
    "- 'cat' --> [-1.1, 2.3, 0.7]\n",
    "- 'apple' --> [3.1, 0.9, -4.7]\n",
    "\n",
    "Here, your embedding space is of size 3.\n",
    "\n",
    "What you expect is to have representation such as words with close meanings are close in this embedding space. As on this example:\n",
    "\n",
    "![Embedding](word_embedding.png)\n",
    "\n",
    "❓ **Question** ❓ Let's run Word2Vec! The following code imports word2vec from gensim, a great python package that makes the use of the word2vec algorithm easy, fast and accurate - which is not an easy task. The second line learns the embedding representation of the words thanks to the sentences in `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(sentences=X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the embedded representation of some words.\n",
    "\n",
    "You can use `word2vec.wv` as a dictionnary.\n",
    "For instance, `word2vec.wv['dog']` will return a representation of `dog` in the embedding space.\n",
    "\n",
    "❓ **Question** ❓ Try different words - especially, try non-existing words to see that they don't have any representation (which is perfectly normal as their representation were not learn). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR WORDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ What is the size of each word representation, and therefore, what is the size of the embedding space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to know if this embedding make any sense? To do that, we will check that words with a close meaning have close representations. \n",
    "\n",
    "Let's use the `most_similar` method that, given an input word, display the \"closest\" words in the embedding space. If the embedding is well done, then words that have close meanings will have close representation in the embedding space.\n",
    "\n",
    "❓ **Question** ❓ Test the `most_similar` method on different words. \n",
    "\n",
    "❗ **Remark** ❗ Indeed, the quality of the closeness will depend on the quality of your embedding, and thus, depend on the number of sentences that you have loaded and from which you create your embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.wv.most_similar(\"car\")\n",
    "\n",
    "### TRY OTHER WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to `most_similar` used on words directly, we can use `similar_by_vector` on vectors to do the same thing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = word2vec.wv['car']\n",
    "word2vec.wv.similar_by_vector(word_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arithmetic on words\n",
    "\n",
    "Now, let's do mathematical operations on words - meaning on their vector representations!\n",
    "\n",
    "As any word is represented as a vector, you can do basic arithmetic as\n",
    "\n",
    "W2V(good) - W2V(bad)\n",
    "\n",
    "❓ **Question** ❓ Do this mathematical operation and print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, image for a second that, somehow, the following equality holds true - just for a second\n",
    "\n",
    "    W2V(good) - W2V(bad) = W2V(nice) - W2V(unpleasant)\n",
    "\n",
    "which is equivalent to \n",
    "\n",
    "    W2V(good) - W2V(bad) + W2V(unpleasant) = W2V(nice).\n",
    "\n",
    "❓ **Question** ❓ Let's, just for fun (as it would be foolish of us to think that this equality holds true ...), do the operation W2V(good) - W2V(bad) + W2V(unpleasant) and store it in a `res` variable (which will be a vector of size 100 that you can print)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We earlier said that, for any vector, it is possible to see the closest vectors in the embedding space.\n",
    "\n",
    "❓ **Question** ❓ Look at the closest vector (thanks to the `word2vec.wv.similar_by_vecto` function) of `res`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incredible right! You can do arithmetical operations on words!\n",
    "\n",
    "❓ **Question** ❓ You can try on arithmetic such as \n",
    "\n",
    "W2V(Boy) - W2V(Girl) = W2V(Man) - W2V(Woman)\n",
    "\n",
    "or \n",
    "\n",
    "W2V(Queen) - W2V(King) = W2V(actress) - W2V(actor)\n",
    "\n",
    "❗ **Remark** ❗ You will probably see that the results are not perfect. But don't forget that you trained your model on a very small corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder where does this magic comes from (at quite a low price, you just run a line of code on a very small corpus and it was trained within few minutes). The magic comes from the way Word2Vec is trained. The details are quite complex, but you can remembed that Word2vec, in `word2vec = Word2Vec(sentences=X_train)` , actually trains a internal neural network (that you don't see).  \n",
    "\n",
    "In a nutshell, this internal NN predicts a word from the surroundings words in a sentences. So it chooses many splits in the different sentences, choose some words as inputs $X$  and a word as output $y$ which it tries to predict, in the embedding space.\n",
    "\n",
    "And as any neural network, Word2Vec has some hyperparameters. Let's check some. \n",
    "\n",
    "\n",
    "# Word2Vec hyperparameters\n",
    "\n",
    "\n",
    "❓ **Question** ❓ The first important hyperparameter is the `size` argument. It corresponds to the size of the embedding space. Learn a new `word2vec_2` model, still trained on the `X_train`, but with a smaller or higher `size`.\n",
    "\n",
    "Verify on some words that the corresponding embedding is of your selected size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Use the `word2vec.wv.vocab` attribute to display the size of the learnt vocabulary. On the other hand, compare it to the number of different words in `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an important difference between the number of words in the train sentences and in the Word2Vec vocabulary, even though it has been train on the train sentence set. The reasons comes from the second important hyperparameter of Word2Vec :  `min_count`. \n",
    "\n",
    "`min_count` is a integer that tells you how many occurences a given word should have to be learn in the embedding space. For instance, let's say that the word \"movie\" appears 1000 times in the corpus and \"simba\" only 2 times. If `min_count=3`, the word \"simba\" will be skipped during the training.\n",
    "\n",
    "The intention is to have only words that are sufficiently present in the corpus to have a robust embedded representation\n",
    "\n",
    "❓ **Question** ❓ Learn a new `word2vec_3` model with a `min_count` higher than 5 (which is the default value) and a `word2vec_4` with a `min_count` smaller than 5, and then, compare the size of the vocabulary for all the different word2vec that you have trained (you can choose any `size` you want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we say that word2vec has an internal neural network that it optimizes based on some predictions? These predictions actually correspond to predicting a word based on surrounding words. The surroundings words are in a `window` which corresponds to the number of words taken into account. And you can train the word2vec with different `window` sizes.\n",
    "\n",
    "❓ **Question** ❓ Learn a new `word2vec_5` model with a `window` different than previously (default is 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments you have seen (`size`, `min_count` and `window`) are usually the one that you should start changing to get a better performance for your model.\n",
    "\n",
    "But you can also look at other arguments in the [documentation](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Text8Corpus)\n",
    "\n",
    "\n",
    "# Convert our train and test set to RNN ready data\n",
    "\n",
    "Now, let's convert the training and test data into their vector representation to be ready to be feed in RNNs.\n",
    "\n",
    "❓ **Question** ❓ Now, write a function that, given a sentence, returns a matrix that corresponds to the embedding of the full sentence, which means that you have to embeed each word one after the other and concatenate the result to output a 2D matrix (be sure that your output is a numpy array)\n",
    "\n",
    "❗ **Remark** ❗ You will probably notice that some words you are trying to convert throw errors as they are said not to belong to the dictionnary:\n",
    "\n",
    "- for the test set, this is understandable: some words were not in the train set and thus their embedded representation is unknwon\n",
    "- for the train set, due to `min_count` hyperparameter, not all the words have a vector representation\n",
    "\n",
    "In any case, just skip the missing words here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ['this', 'movie', 'is', 'the', 'worst', 'action', 'movie', 'ever']\n",
    "example_missing_words = ['this', 'movie', 'is', 'laaaaaaaaaame']\n",
    "\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "### Checks\n",
    "embedded_sentence = embed_sentence(word2vec, example)\n",
    "assert(type(embedded_sentence) == np.ndarray)\n",
    "assert(embedded_sentence.shape == (8, 100))\n",
    "\n",
    "embedded_sentence_missing_words = embed_sentence(word2vec, example_missing_words)  \n",
    "assert(type(embedded_sentence_missing_words) == np.ndarray)\n",
    "assert(embedded_sentence_missing_words.shape == (3, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Write a function that, given a list of sentence (each sentence being a list of words/strings), returns a list of embedded sentences (each sentence is a matrix). Apply this function to the train and test sentences\n",
    "\n",
    "Hint: Use the previous function `embed_sentence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(word2vec, sentences):\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "X_train = embedding(word2vec, X_train)\n",
    "X_test = embedding(word2vec, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ In order to have ready-to-use adta, do not forget to pad them, as yesterday, in order to have tensors that can be divided in batch sizes during the optimization. Store the padedd values in `X_train_pad` and `X_test_pad`. Do not forget the important arguments of the padding ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "assert(len(X_train_pad.shape) == 3)\n",
    "assert(len(X_test_pad.shape) == 3)\n",
    "assert(X_train_pad.shape[2] == 100)\n",
    "assert(X_test_pad.shape[2] == 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load existing models: transfer learning\n",
    "\n",
    "There is a possibility that your computer cannot train Word2Vec on large corpus (or just takes too much time, or just that you want a word2vec trained on a very large corpus). In that case, you can directly load word2Vec embedding that have been preprained on other corpuses. This is another example of transfer learning.\n",
    "\n",
    "❓ **Question** ❓ To do that, look at available corpuses thanks to the following command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "print(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Load a pretrained model thanks to `api.load(name_of_your_corpus)` and check the size of the vocabulary, but also the size of the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
