{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning week - Day 1 - Your First Neural Network\n",
    "\n",
    "### Exercise objectives\n",
    "- Write your first Neural Network\n",
    "- Inspect some of the most important hyperparameters of Neural Networks\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "\n",
    "In this exercise, you will build your first Neural Network that will separate two classes.\n",
    "Each data $X$ has only two coordinate $X = (x_1, x_2)$ and belongs to the class 0 or to the class 1. It is called the two moons dataset. \n",
    "\n",
    "As there is only two features, it can be represented on a graph, where the color corresponds to one of the two classes. Here is an example of such data : \n",
    "\n",
    "![Two moons](moons_example.png)\n",
    "\n",
    "\n",
    "\n",
    "# Create data\n",
    "\n",
    "Here, we will use the scikit-learn `make_moons` function [(see documentation here)](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) that creates 2 moons that cannot be linearly separated. Each moon correspond to a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "### Data generation\n",
    "X, y = make_moons(n_samples=300, noise=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function allows to plot the two moons\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_moons(X, y):\n",
    "    df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "    colors = {0:'red', 1:'blue'}\n",
    "    fig, ax = plt.subplots()\n",
    "    grouped = df.groupby('label')\n",
    "\n",
    "    for key, group in grouped:\n",
    "        group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "    plt.show()\n",
    "\n",
    "plot_moons(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Play with the number of samples and the noise to see the effect on the data, by plotting the moons for different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Draw 250 samples of the data with a noise equal to 0.20 (random state being 0) and split the initial dataset into a train and test set (size: 70/30%)\n",
    "\n",
    "Remark : Please call the variables `X_train`, `y_train`, `X_test` and `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now define your first neural network.\n",
    "The architecture of your model should be : \n",
    "- a first layer with 5 neurons, with activation function being `relu` and the correct input dimension\n",
    "- a output layer suited to your 2-class classification problem.\n",
    "\n",
    "\n",
    "❓ Question ❓ Complete the next function (in the `#TODO`) with the previous architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    \n",
    "    #Model architecture\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    #Model optimization : Optimizer, loss and metric    \n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗ **Remark** ❗ Briefly speaking, the compile tells your algorithm how to optimize the weights of your network once it will be fit onto real data. We will see it in depth tomorrow.\n",
    "\n",
    "❓ **Question** ❓ Oh, by the way, how many parameters does the model have? Once your model is initialized, you can use the `summary` function that display the stack of layers, the shape of the output after each layer, and the number of parameters of each layer. Use this function to check that you have 21 parameters i.e. weights in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to train your algorithm on some data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=30, \n",
    "                    batch_size=16, \n",
    "                    verbose=0)\n",
    "\n",
    "### Do not change the epochs and batch size for now - we will see it tomorrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Try different verbose value to see what the model output during the 100 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following function to vizualise the loss value during the iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `evaluate` method of `model` evaluate your model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_train, y_train, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`results` is a list where the first element corresponds to the loss value. In the defined model, we use `binary_crossentropy` (see `model.compile(...)` above). This value is hard to interpreted, which is the reason why `model.compile` takes a `metrics` keyword to estimate other metrics. Those metrics are compute in the `evaluate` function and corresponds to the next values in the list.\n",
    "\n",
    "\n",
    "❓ **Question** ❓ Based on the value of the `results`, what is the accuracy on the train set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not a good idea to evaluate a model on the train set. Therefore, what is your accuracy on the test test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗ **Remark** ❗ The number of iterations has been selected quite arbitrary : it corresponds to the `epochs` keyword in the `model.fit(...)` method (that we will see in-depth tomorrow).\n",
    "\n",
    "❓ **Question** ❓ Copy paste the function above to relaunch the model on 500 iterations (DO NOT FORGET TO CALL the `initialize_model` function, otherwise, your initial parameters are not random but those that you already learnt on the previous fit!!). Plot the history to see how the loss changed over the different epochs/iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ In this scenario, what is the accuracy on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen here that the more epochs, the better. This is not always true. So when to stop? The answer will be seen tomorrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Complete this new initialization of the model with a deeper architecture that includes : \n",
    "\n",
    "- a first layer with 10 neurons (activation being relu)\n",
    "- a second layer with 5 neurons (activation being relu)\n",
    "- an output layer suited for this problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_2():\n",
    "\n",
    "    # Model architecture    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Model optimization : Optimizer, loss and metric    \n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ What is the number of parameters of your new model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Run your model on the previous data with 500 epochs and plot the loss afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ What is your accuracy on the test test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Open question** ❓ You can try different  number of layers, different number of neurons, ... feel free to test different architecture :)\n",
    "\n",
    "❗ **Remark** ❗ Are you sure you want an accuracy of 100% with this amount of noise? Maybe your architecture is good enough as it is and getting 100% would only be overfitting ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
