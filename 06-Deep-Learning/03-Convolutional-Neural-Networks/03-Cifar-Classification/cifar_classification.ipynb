{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning week - Day 3 - CIFAR Classification\n",
    "\n",
    "### Exercise objectives\n",
    "- Implement a CNN for a 10-class classification problem\n",
    "- Enhance the CNN performance with data augmentation techniques\n",
    "- Experiment the limitations of training network with large images on your computer (more especially on your CPU)\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "You should now have a better feeling of how a CNN is working, and especially how the convolutions are affecting the image to detect specific features. Therefore, let's now play with a bit more complex images. The CIFAR-10 dataset is a dataset that contains images of 10 different classes : \n",
    "- airplane \t\t\t\t\t\t\t\t\t\t\n",
    "- automobile \t\t\t\t\t\t\t\t\t\t\n",
    "- bird \t\t\t\t\t\t\t\t\t\t\n",
    "- cat \t\t\t\t\t\t\t\t\t\t\n",
    "- deer \t\t\t\t\t\t\t\t\t\t\n",
    "- dog \t\t\t\t\t\t\t\t\t\t\n",
    "- frog \t\t\t\t\t\t\t\t\t\t\n",
    "- horse \t\t\t\t\t\t\t\t\t\t\n",
    "- ship \t\t\t\t\t\t\t\t\t\t\n",
    "- truck\n",
    "\n",
    "This dataset is emblematic in the research community as many enhancements for image problems have been achieved on this dataset, and later on the CIFAR-100 dataset once the performance got too high. You can check the [wikipedia](https://en.wikipedia.org/wiki/CIFAR-10) page of the dataset if you want to know more about it.\n",
    "\n",
    "In this notebook, we propose to implement a CNN to distinguish the 10 categories from the CIFAR-10 dataset. Again, remember that until 10 years ago, this problem was very challenging to the entire research community and is now for you to tackle.\n",
    "\n",
    "\n",
    "⚠️ **Warning** ⚠️ In this exercise, computations are done on your computer, and most probably on your CPU. Therefore, bear in mind that a model training will take ~10 minutes on the entire dataset. You will here experiment the fact that these computations are heavy and requires a lot of computational power. We will see in the next exercise how to overcome this problem. Until then, there is a trick that you can use in any ML problem if you are still in the experimental / design part but face long waiting times : select a subset of your data (for instance `X_train = X_train[:100]`) to code all the different parts without taking care of the performance. Once the code is ready, you can seek for performance by taking the entire dataset.\n",
    "\n",
    "\n",
    "\n",
    "# Data\n",
    "\n",
    "❓ **Question** ❓ To load the CIFAR-10 dataset you can use `keras` package directly (see [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10)). What is the shape of the images? How many images do you have per class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "(X_train, labels_train), (X_test, labels_test) = cifar10.load_data()\n",
    "\n",
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ To ease the convergence of the algorithm, it is usefull to normalize the data. See here what are the maximum and minimum values in your data, and normalize it accordingly (the resulting image intensities should be between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Display some of the images and their relative class. You can use the `imshow` function from matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "### Answer ###\n",
    "##############\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for i in range(5):\n",
    "    img = X_train[i]\n",
    "    label = labels_train[i][0]\n",
    "    \n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.imshow(img)\n",
    "    plt.title(labels[label])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ The labels (`labels_train` and `labels_test`) are stored as list of integers. Convert it to one-hot encoded labels so that they can be used to train a classification neural network. You can use the `to_categorical` function from Keras. Store the the categories in `y_train` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "Now, let's define the Convolutional Neural Network. \n",
    "\n",
    "❓ **Question** ❓ Define a CNN that is composed of:\n",
    "- a Conv2D layer with 32 filters, a kernel size of (3, 3), the relu activation function, and a padding equal to `same`\n",
    "- a MaxPooling2D layer with a pool size of (2, 2)\n",
    "- a Conv2D layer with 64 filters, a kernel size of (3, 3), the relu activation function, and a padding equal to `same`\n",
    "- a MaxPooling2D layer with a pool size of (2, 2)\n",
    "- a Conv2D layer with 128 filters, a kernel size of (3, 3), the relu activation function, and a padding equal to `same`\n",
    "- a MaxPooling2D layer with a pool size of (3, 3)\n",
    "- a Flatten layer\n",
    "- a dense function with 120 neurons with the `relu` activation function\n",
    "- a dense function with 60 neurons with the `relu` activation function\n",
    "- a dropout layer (with a rate of 0.5), to regularize the network\n",
    "- a dense function related to your task\n",
    " \n",
    " ⚠️ **Warning** ⚠️  Do not include the compilation in the function.\n",
    " \n",
    " ⚠️ **Warning** ⚠️ Do not forget to add the input shape of your data to the first layer. And do not forget that it has three colors ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ What is the number of parameters of your model? \n",
    "\n",
    "<details>\n",
    "   <summary>If you don't remember how to check the number of parameters, click >>here<<</summary>\n",
    "    `model.summary()`\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Write a function to compile your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Compile your model and fit it on your training data, with an early stopping (patience to 5 to keep fast computations).\n",
    "\n",
    "Store the output of the fit in an `history` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Run the following function on the previous history (keep the default arguments, these are intended for future plots in the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, title='', axs=None, exp_name=\"\"):\n",
    "    if axs is not None:\n",
    "        ax1, ax2 = axs\n",
    "    else:\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    if len(exp_name) > 0 and exp_name[0] != '_':\n",
    "        exp_name = '_' + exp_name\n",
    "    ax1.plot(history.history['loss'], label='train' + exp_name)\n",
    "    ax1.plot(history.history['val_loss'], label='val' + exp_name)\n",
    "    ax1.set_ylim(0., 2.2)\n",
    "    ax1.set_title('loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(history.history['accuracy'], label='train accuracy'  + exp_name)\n",
    "    ax2.plot(history.history['val_accuracy'], label='val accuracy'  + exp_name)\n",
    "    ax2.set_ylim(0.25, 1.)\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.legend()\n",
    "    return (ax1, ax2)\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Evaluate your model on the test data. Are you satisfied with these performances ? What is the chance level on this task ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "\n",
    "To easily improve the accuracy of a model without much work, we can generate new data: the _data augmentation_. This widely used technique consists in applying little transformation to input images without changing its label, as mirroring, cropping, intensity changes, etc. The improved performance simply results from the Neural network training with more different data.\n",
    "\n",
    "The natural way to generate these new images is to apply some transformations and train the model on the original and new images. However, such procedure requires to keep all these images in memory : it can be very intensive, to the point that your computer memory cannot hold any new image (your computer might even crash).\n",
    "\n",
    "For this reason, we will augment the data _on the fly_, meaning that we will create new data, use them to fit the model, then delete them. Here, Keras is our friend as it provides the utils to do all this job for us. Look at the following code : the general writing can seem odd but don't be panicked: just look at the function arguments that defines the augmentation techniques that we will use and that you can check in the  [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=(0.8, 1.2),) \n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can vizualize the input image and the transformed one with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_augmented = datagen.flow(X_train, shuffle=False, batch_size=1)\n",
    "\n",
    "for i, (raw_image, augmented_image) in enumerate(zip(X_train, X_augmented)):\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 2))\n",
    "    ax1.imshow(raw_image)\n",
    "    ax2.imshow(augmented_image[0])\n",
    "    plt.show()\n",
    "    \n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗ **Remark** ❗ In this example, there is one augmented image per initial image. In fact, it is possible to have many augmented images per initial image.\n",
    "\n",
    "❓ **Question** ❓ Previously, we used the `validation_split` argument to let the model separate a training set from the validation one. It is not possible here as using an image in the training set and its transformation in the validation set is considered as a data leakage. Therefore, we have to manually define the `validation_data` with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model\n",
    "model_2 = initialize_model()\n",
    "model_2 = compile_model(model_2)\n",
    "\n",
    "# The data generator\n",
    "X_tr = X_train[:40000]\n",
    "y_tr = y_train[:40000]\n",
    "X_val = X_train[40000:]\n",
    "y_val = y_train[40000:]\n",
    "train_flow = datagen.flow(X_tr, y_tr, batch_size=32)\n",
    "\n",
    "# The early stopping criterion\n",
    "es = EarlyStopping(patience=5)\n",
    "\n",
    "# The fit\n",
    "history_2 = model_2.fit(train_flow, \n",
    "                        epochs=100, \n",
    "                        callbacks=[es], \n",
    "                        validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark: The training can be quite long here. You can go to the next exercise and gome back once in a while to finish the last questions\n",
    "\n",
    "❓ **Question** ❓ Now, let's plot the previous and current run histories. What do you think of the data augmentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs = plot_history(history_2, exp_name='data_augmentation')\n",
    "plot_history(history ,axs=axs, exp_name='baseline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Evaluate the model on the test data. Do you see an improvement ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Remark\n",
    "\n",
    "One thing you have probably noticed in this notebook is that the training is quite long. This is the reason why we stopped training quickly to still have somehow fast experiments. However, in practice, training must be allowed to last longer, with a a stopping criterion that has a lower delta and higher patience!\n",
    "\n",
    "How can we do that?  Actually, when you run the notebook on your compute, you train the neural network on your CPU. However, training neural network on images (in each batch) can be parallelized, and this parallelization procedure can be done on GPU.\n",
    "\n",
    "First, you might face the fact that you don't have a GPU on your computer. Bur more importantly, it can be hard to set up the training on the GPU as it requires special hardware, software and sometimes OS. Therefore, we will look at another way to train our CNN on GPU (for free): Google Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
